{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "bb6fc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce18043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "6d015d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Precily_Text_Similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "32da227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>uk directors guild nominees named martin scors...</td>\n",
       "      <td>steel firm  to cut  45 000 jobs mittal steel  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "      <td>israel looks to us for bank chief israel has a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>pountney handed ban and fine northampton coach...</td>\n",
       "      <td>india and iran in gas export deal india has si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>belle named  best scottish band  belle &amp; sebas...</td>\n",
       "      <td>mido makes third apology ahmed  mido  hossam h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>criminal probe on citigroup deals traders at u...</td>\n",
       "      <td>former ni minister scott dies former northern ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text1  \\\n",
       "0     broadband challenges tv viewing the number of ...   \n",
       "1     rap boss arrested over drug find rap mogul mar...   \n",
       "2     player burn-out worries robinson england coach...   \n",
       "3     hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4     sir paul rocks super bowl crowds sir paul mcca...   \n",
       "...                                                 ...   \n",
       "2995  uk directors guild nominees named martin scors...   \n",
       "2996  u2 to play at grammy awards show irish rock ba...   \n",
       "2997  pountney handed ban and fine northampton coach...   \n",
       "2998  belle named  best scottish band  belle & sebas...   \n",
       "2999  criminal probe on citigroup deals traders at u...   \n",
       "\n",
       "                                                  text2  \n",
       "0     gardener wins double in glasgow britain s jaso...  \n",
       "1     amnesty chief laments war failure the lack of ...  \n",
       "2     hanks greeted at wintry premiere hollywood sta...  \n",
       "3     redford s vision of sundance despite sporting ...  \n",
       "4     mauresmo opens with victory in la amelie maure...  \n",
       "...                                                 ...  \n",
       "2995  steel firm  to cut  45 000 jobs mittal steel  ...  \n",
       "2996  israel looks to us for bank chief israel has a...  \n",
       "2997  india and iran in gas export deal india has si...  \n",
       "2998  mido makes third apology ahmed  mido  hossam h...  \n",
       "2999  former ni minister scott dies former northern ...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = data.iloc[[0]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "7b15a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def stemming(text):   # defining a function stemming\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    # it removes the other characters rather than alphabetical and replace with a white space \n",
    "    text = text.lower()  \n",
    "    # transforming the words into lowercase that is because stopwords are in lowercase, so that why any stopwords present\n",
    "    # in the text have not left to remove because of uppercase.\n",
    "    text = nltk.word_tokenize(text)\n",
    "    # Creating tokens\n",
    "    text = ' '.join([lemma.lemmatize(word) for word in text if not word in stopwords.words('english')])\n",
    "    # removing the stopwords if any and then lemmatize the tokens and then join them to form a single text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c9c047ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemmed_text1'] = data.text1.apply(lambda x: stemming(x))\n",
    "data['stemmed_text2'] = data.text2.apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "914c214a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>stemmed_text1</th>\n",
       "      <th>stemmed_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "      <td>broadband challenge tv viewing number european...</td>\n",
       "      <td>gardener win double glasgow britain jason gard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "      <td>rap bos arrested drug find rap mogul marion su...</td>\n",
       "      <td>amnesty chief lament war failure lack public o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "      <td>player burn worry robinson england coach andy ...</td>\n",
       "      <td>hank greeted wintry premiere hollywood star to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "      <td>heart oak cotonsport heart oak set ghanaian co...</td>\n",
       "      <td>redford vision sundance despite sporting cordu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "      <td>sir paul rock super bowl crowd sir paul mccart...</td>\n",
       "      <td>mauresmo open victory la amelie mauresmo maria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>uk directors guild nominees named martin scors...</td>\n",
       "      <td>steel firm  to cut  45 000 jobs mittal steel  ...</td>\n",
       "      <td>uk director guild nominee named martin scorses...</td>\n",
       "      <td>steel firm cut job mittal steel one world larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>u2 to play at grammy awards show irish rock ba...</td>\n",
       "      <td>israel looks to us for bank chief israel has a...</td>\n",
       "      <td>u play grammy award show irish rock band u pla...</td>\n",
       "      <td>israel look u bank chief israel asked u banker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>pountney handed ban and fine northampton coach...</td>\n",
       "      <td>india and iran in gas export deal india has si...</td>\n",
       "      <td>pountney handed ban fine northampton coach bud...</td>\n",
       "      <td>india iran gas export deal india signed bn bn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>belle named  best scottish band  belle &amp; sebas...</td>\n",
       "      <td>mido makes third apology ahmed  mido  hossam h...</td>\n",
       "      <td>belle named best scottish band belle sebastian...</td>\n",
       "      <td>mido make third apology ahmed mido hossam made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>criminal probe on citigroup deals traders at u...</td>\n",
       "      <td>former ni minister scott dies former northern ...</td>\n",
       "      <td>criminal probe citigroup deal trader u banking...</td>\n",
       "      <td>former ni minister scott dy former northern ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text1  \\\n",
       "0     broadband challenges tv viewing the number of ...   \n",
       "1     rap boss arrested over drug find rap mogul mar...   \n",
       "2     player burn-out worries robinson england coach...   \n",
       "3     hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4     sir paul rocks super bowl crowds sir paul mcca...   \n",
       "...                                                 ...   \n",
       "2995  uk directors guild nominees named martin scors...   \n",
       "2996  u2 to play at grammy awards show irish rock ba...   \n",
       "2997  pountney handed ban and fine northampton coach...   \n",
       "2998  belle named  best scottish band  belle & sebas...   \n",
       "2999  criminal probe on citigroup deals traders at u...   \n",
       "\n",
       "                                                  text2  \\\n",
       "0     gardener wins double in glasgow britain s jaso...   \n",
       "1     amnesty chief laments war failure the lack of ...   \n",
       "2     hanks greeted at wintry premiere hollywood sta...   \n",
       "3     redford s vision of sundance despite sporting ...   \n",
       "4     mauresmo opens with victory in la amelie maure...   \n",
       "...                                                 ...   \n",
       "2995  steel firm  to cut  45 000 jobs mittal steel  ...   \n",
       "2996  israel looks to us for bank chief israel has a...   \n",
       "2997  india and iran in gas export deal india has si...   \n",
       "2998  mido makes third apology ahmed  mido  hossam h...   \n",
       "2999  former ni minister scott dies former northern ...   \n",
       "\n",
       "                                          stemmed_text1  \\\n",
       "0     broadband challenge tv viewing number european...   \n",
       "1     rap bos arrested drug find rap mogul marion su...   \n",
       "2     player burn worry robinson england coach andy ...   \n",
       "3     heart oak cotonsport heart oak set ghanaian co...   \n",
       "4     sir paul rock super bowl crowd sir paul mccart...   \n",
       "...                                                 ...   \n",
       "2995  uk director guild nominee named martin scorses...   \n",
       "2996  u play grammy award show irish rock band u pla...   \n",
       "2997  pountney handed ban fine northampton coach bud...   \n",
       "2998  belle named best scottish band belle sebastian...   \n",
       "2999  criminal probe citigroup deal trader u banking...   \n",
       "\n",
       "                                          stemmed_text2  \n",
       "0     gardener win double glasgow britain jason gard...  \n",
       "1     amnesty chief lament war failure lack public o...  \n",
       "2     hank greeted wintry premiere hollywood star to...  \n",
       "3     redford vision sundance despite sporting cordu...  \n",
       "4     mauresmo open victory la amelie mauresmo maria...  \n",
       "...                                                 ...  \n",
       "2995  steel firm cut job mittal steel one world larg...  \n",
       "2996  israel look u bank chief israel asked u banker...  \n",
       "2997  india iran gas export deal india signed bn bn ...  \n",
       "2998  mido make third apology ahmed mido hossam made...  \n",
       "2999  former ni minister scott dy former northern ir...  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "38c2f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemmed_text1'] = data.stemmed_text1.apply(lambda x: nltk.word_tokenize(x))\n",
    "data['stemmed_text2'] = data.stemmed_text2.apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "76accd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(len(data.stemmed_text1[0]+data.stemmed_text2[0])/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7c1945a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = data.stemmed_text1.to_list() + data.stemmed_text2.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "67c7d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "b59d1f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "46815bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Set values for various parameters\n",
    "# feature_size = 50    # Word vector dimensionality\n",
    "# min_word_count = 1\n",
    "# ft_model = Word2Vec(corpus, vector_size=feature_size, min_count=min_word_count)\n",
    "# # ft_model2 = FastText(data.stemmed_text2[0], vector_size=feature_size, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2c13d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "26462e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_embedded.pkl', 'wb') as f:\n",
    "    pickle.dump(ft_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "2228ee85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c587901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "83c13d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(ft_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "269e8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "97872e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0e0e0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get document level embeddings\n",
    "# ft_text1_features = averaged_word_vectorizer(corpus=data.stemmed_text1.to_list(), model=ft_model, num_features=feature_size)\n",
    "# ft_text2_features = averaged_word_vectorizer(corpus=data.stemmed_text2.to_list(), model=ft_model, num_features=feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "15eec87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_text2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "f0dbb454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['text1_vec'] = [i for i in ft_text1_features]\n",
    "data['text2_vec'] = [i for i in ft_text2_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "327979f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text1_vec'] = data['stemmed_text1'].apply(lambda x: vectorizer.fit_transform(x))\n",
    "data['text2_vec'] = data['stemmed_text2'].apply(lambda x: vectorizer.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "10cfb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    \"\"\"\n",
    "    dot = round(np.dot(A, B), 3)\n",
    "    norma = round(np.sqrt(np.dot(A, A)), 3)\n",
    "    normb = round(np.sqrt(np.dot(B, B)), 3)\n",
    "    cos = round(dot / (norma * normb), 3)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "9f47bb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[489], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cosine_similarity(data\u001b[38;5;241m.\u001b[39mtext1_vec[\u001b[38;5;241m0\u001b[39m], data\u001b[38;5;241m.\u001b[39mtext2_vec[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[472], line 9\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(A, B)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_similarity\u001b[39m(A, B):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m        A: a numpy array which corresponds to a word vector\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m        cos: numerical number representing the cosine similarity between A and B.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mdot(A, B), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     10\u001b[0m     norma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(A, A)), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     11\u001b[0m     normb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdot(B, B)), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:590\u001b[0m, in \u001b[0;36mspmatrix.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_dispatch(other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:540\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(other):\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 540\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_sparse_matrix(other)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "cosine_similarity(data.text1_vec[0], data.text2_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "92e01ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.433"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(data.text1_vec.iloc[0], data.text2_vec.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "21ae7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "simi_results = []\n",
    "for i in range(data.shape[0]):\n",
    "    simi_results.append(cosine_similarity(data.text1_vec.iloc[i], data.text2_vec.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "5787ced5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.096,\n",
       " 0.115,\n",
       " 0.123,\n",
       " 0.142,\n",
       " 0.146,\n",
       " 0.151,\n",
       " 0.152,\n",
       " 0.158,\n",
       " 0.163,\n",
       " 0.167,\n",
       " 0.17,\n",
       " 0.172,\n",
       " 0.173,\n",
       " 0.173,\n",
       " 0.174,\n",
       " 0.174,\n",
       " 0.174,\n",
       " 0.177,\n",
       " 0.182,\n",
       " 0.183,\n",
       " 0.186,\n",
       " 0.187,\n",
       " 0.19,\n",
       " 0.191,\n",
       " 0.192,\n",
       " 0.192,\n",
       " 0.195,\n",
       " 0.196,\n",
       " 0.198,\n",
       " 0.199,\n",
       " 0.201,\n",
       " 0.202,\n",
       " 0.202,\n",
       " 0.203,\n",
       " 0.203,\n",
       " 0.205,\n",
       " 0.205,\n",
       " 0.206,\n",
       " 0.207,\n",
       " 0.208,\n",
       " 0.208,\n",
       " 0.209,\n",
       " 0.21,\n",
       " 0.21,\n",
       " 0.21,\n",
       " 0.211,\n",
       " 0.214,\n",
       " 0.217,\n",
       " 0.223,\n",
       " 0.223,\n",
       " 0.224,\n",
       " 0.224,\n",
       " 0.224,\n",
       " 0.225,\n",
       " 0.225,\n",
       " 0.225,\n",
       " 0.226,\n",
       " 0.227,\n",
       " 0.227,\n",
       " 0.228,\n",
       " 0.228,\n",
       " 0.229,\n",
       " 0.23,\n",
       " 0.23,\n",
       " 0.232,\n",
       " 0.235,\n",
       " 0.235,\n",
       " 0.235,\n",
       " 0.236,\n",
       " 0.236,\n",
       " 0.237,\n",
       " 0.237,\n",
       " 0.237,\n",
       " 0.239,\n",
       " 0.239,\n",
       " 0.239,\n",
       " 0.24,\n",
       " 0.24,\n",
       " 0.241,\n",
       " 0.241,\n",
       " 0.242,\n",
       " 0.243,\n",
       " 0.243,\n",
       " 0.244,\n",
       " 0.245,\n",
       " 0.245,\n",
       " 0.246,\n",
       " 0.246,\n",
       " 0.246,\n",
       " 0.248,\n",
       " 0.248,\n",
       " 0.249,\n",
       " 0.249,\n",
       " 0.249,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.252,\n",
       " 0.252,\n",
       " 0.253,\n",
       " 0.253,\n",
       " 0.253,\n",
       " 0.253,\n",
       " 0.253,\n",
       " 0.253,\n",
       " 0.256,\n",
       " 0.256,\n",
       " 0.256,\n",
       " 0.256,\n",
       " 0.257,\n",
       " 0.257,\n",
       " 0.257,\n",
       " 0.257,\n",
       " 0.257,\n",
       " 0.258,\n",
       " 0.259,\n",
       " 0.259,\n",
       " 0.26,\n",
       " 0.26,\n",
       " 0.261,\n",
       " 0.261,\n",
       " 0.262,\n",
       " 0.262,\n",
       " 0.262,\n",
       " 0.262,\n",
       " 0.262,\n",
       " 0.263,\n",
       " 0.264,\n",
       " 0.265,\n",
       " 0.266,\n",
       " 0.266,\n",
       " 0.267,\n",
       " 0.267,\n",
       " 0.268,\n",
       " 0.268,\n",
       " 0.268,\n",
       " 0.269,\n",
       " 0.269,\n",
       " 0.269,\n",
       " 0.27,\n",
       " 0.27,\n",
       " 0.27,\n",
       " 0.271,\n",
       " 0.272,\n",
       " 0.273,\n",
       " 0.274,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.275,\n",
       " 0.276,\n",
       " 0.277,\n",
       " 0.277,\n",
       " 0.277,\n",
       " 0.277,\n",
       " 0.278,\n",
       " 0.278,\n",
       " 0.278,\n",
       " 0.278,\n",
       " 0.279,\n",
       " 0.279,\n",
       " 0.279,\n",
       " 0.279,\n",
       " 0.28,\n",
       " 0.28,\n",
       " 0.28,\n",
       " 0.281,\n",
       " 0.283,\n",
       " 0.283,\n",
       " 0.283,\n",
       " 0.283,\n",
       " 0.284,\n",
       " 0.284,\n",
       " 0.285,\n",
       " 0.285,\n",
       " 0.286,\n",
       " 0.287,\n",
       " 0.288,\n",
       " 0.288,\n",
       " 0.288,\n",
       " 0.289,\n",
       " 0.289,\n",
       " 0.29,\n",
       " 0.29,\n",
       " 0.291,\n",
       " 0.292,\n",
       " 0.292,\n",
       " 0.293,\n",
       " 0.293,\n",
       " 0.293,\n",
       " 0.293,\n",
       " 0.293,\n",
       " 0.294,\n",
       " 0.294,\n",
       " 0.294,\n",
       " 0.295,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.296,\n",
       " 0.297,\n",
       " 0.297,\n",
       " 0.297,\n",
       " 0.297,\n",
       " 0.298,\n",
       " 0.298,\n",
       " 0.299,\n",
       " 0.299,\n",
       " 0.299,\n",
       " 0.3,\n",
       " 0.3,\n",
       " 0.3,\n",
       " 0.301,\n",
       " 0.301,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.302,\n",
       " 0.303,\n",
       " 0.303,\n",
       " 0.303,\n",
       " 0.304,\n",
       " 0.304,\n",
       " 0.305,\n",
       " 0.305,\n",
       " 0.305,\n",
       " 0.307,\n",
       " 0.307,\n",
       " 0.307,\n",
       " 0.307,\n",
       " 0.308,\n",
       " 0.308,\n",
       " 0.308,\n",
       " 0.308,\n",
       " 0.308,\n",
       " 0.309,\n",
       " 0.309,\n",
       " 0.309,\n",
       " 0.309,\n",
       " 0.309,\n",
       " 0.31,\n",
       " 0.31,\n",
       " 0.311,\n",
       " 0.311,\n",
       " 0.311,\n",
       " 0.311,\n",
       " 0.312,\n",
       " 0.312,\n",
       " 0.314,\n",
       " 0.314,\n",
       " 0.314,\n",
       " 0.314,\n",
       " 0.314,\n",
       " 0.315,\n",
       " 0.315,\n",
       " 0.315,\n",
       " 0.316,\n",
       " 0.317,\n",
       " 0.317,\n",
       " 0.317,\n",
       " 0.317,\n",
       " 0.318,\n",
       " 0.318,\n",
       " 0.318,\n",
       " 0.318,\n",
       " 0.318,\n",
       " 0.318,\n",
       " 0.319,\n",
       " 0.319,\n",
       " 0.32,\n",
       " 0.32,\n",
       " 0.32,\n",
       " 0.32,\n",
       " 0.321,\n",
       " 0.321,\n",
       " 0.321,\n",
       " 0.321,\n",
       " 0.321,\n",
       " 0.322,\n",
       " 0.322,\n",
       " 0.322,\n",
       " 0.323,\n",
       " 0.324,\n",
       " 0.324,\n",
       " 0.324,\n",
       " 0.324,\n",
       " 0.325,\n",
       " 0.325,\n",
       " 0.325,\n",
       " 0.325,\n",
       " 0.326,\n",
       " 0.326,\n",
       " 0.326,\n",
       " 0.326,\n",
       " 0.327,\n",
       " 0.327,\n",
       " 0.328,\n",
       " 0.328,\n",
       " 0.328,\n",
       " 0.328,\n",
       " 0.328,\n",
       " 0.328,\n",
       " 0.329,\n",
       " 0.33,\n",
       " 0.331,\n",
       " 0.331,\n",
       " 0.332,\n",
       " 0.332,\n",
       " 0.332,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.333,\n",
       " 0.334,\n",
       " 0.334,\n",
       " 0.334,\n",
       " 0.335,\n",
       " 0.335,\n",
       " 0.335,\n",
       " 0.335,\n",
       " 0.336,\n",
       " 0.336,\n",
       " 0.336,\n",
       " 0.336,\n",
       " 0.337,\n",
       " 0.337,\n",
       " 0.337,\n",
       " 0.337,\n",
       " 0.337,\n",
       " 0.338,\n",
       " 0.34,\n",
       " 0.34,\n",
       " 0.34,\n",
       " 0.341,\n",
       " 0.341,\n",
       " 0.342,\n",
       " 0.342,\n",
       " 0.342,\n",
       " 0.342,\n",
       " 0.342,\n",
       " 0.343,\n",
       " 0.343,\n",
       " 0.343,\n",
       " 0.343,\n",
       " 0.344,\n",
       " 0.344,\n",
       " 0.344,\n",
       " 0.344,\n",
       " 0.344,\n",
       " 0.345,\n",
       " 0.345,\n",
       " 0.345,\n",
       " 0.346,\n",
       " 0.346,\n",
       " 0.346,\n",
       " 0.346,\n",
       " 0.346,\n",
       " 0.347,\n",
       " 0.347,\n",
       " 0.347,\n",
       " 0.347,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.348,\n",
       " 0.349,\n",
       " 0.349,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.35,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.351,\n",
       " 0.352,\n",
       " 0.352,\n",
       " 0.352,\n",
       " 0.352,\n",
       " 0.352,\n",
       " 0.353,\n",
       " 0.353,\n",
       " 0.353,\n",
       " 0.353,\n",
       " 0.353,\n",
       " 0.354,\n",
       " 0.354,\n",
       " 0.354,\n",
       " 0.354,\n",
       " 0.354,\n",
       " 0.354,\n",
       " 0.355,\n",
       " 0.356,\n",
       " 0.356,\n",
       " 0.356,\n",
       " 0.357,\n",
       " 0.357,\n",
       " 0.357,\n",
       " 0.357,\n",
       " 0.358,\n",
       " 0.358,\n",
       " 0.358,\n",
       " 0.359,\n",
       " 0.359,\n",
       " 0.359,\n",
       " 0.359,\n",
       " 0.36,\n",
       " 0.36,\n",
       " 0.36,\n",
       " 0.36,\n",
       " 0.361,\n",
       " 0.361,\n",
       " 0.361,\n",
       " 0.361,\n",
       " 0.361,\n",
       " 0.361,\n",
       " 0.362,\n",
       " 0.362,\n",
       " 0.362,\n",
       " 0.362,\n",
       " 0.363,\n",
       " 0.363,\n",
       " 0.363,\n",
       " 0.363,\n",
       " 0.363,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.364,\n",
       " 0.365,\n",
       " 0.365,\n",
       " 0.365,\n",
       " 0.365,\n",
       " 0.365,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.366,\n",
       " 0.367,\n",
       " 0.367,\n",
       " 0.367,\n",
       " 0.367,\n",
       " 0.368,\n",
       " 0.368,\n",
       " 0.369,\n",
       " 0.369,\n",
       " 0.369,\n",
       " 0.37,\n",
       " 0.37,\n",
       " 0.37,\n",
       " 0.37,\n",
       " 0.371,\n",
       " 0.371,\n",
       " 0.371,\n",
       " 0.371,\n",
       " 0.371,\n",
       " 0.371,\n",
       " 0.372,\n",
       " 0.372,\n",
       " 0.372,\n",
       " 0.372,\n",
       " 0.372,\n",
       " 0.372,\n",
       " 0.373,\n",
       " 0.373,\n",
       " 0.373,\n",
       " 0.374,\n",
       " 0.374,\n",
       " 0.374,\n",
       " 0.374,\n",
       " 0.375,\n",
       " 0.375,\n",
       " 0.375,\n",
       " 0.376,\n",
       " 0.377,\n",
       " 0.377,\n",
       " 0.377,\n",
       " 0.377,\n",
       " 0.377,\n",
       " 0.378,\n",
       " 0.378,\n",
       " 0.378,\n",
       " 0.378,\n",
       " 0.378,\n",
       " 0.379,\n",
       " 0.379,\n",
       " 0.379,\n",
       " 0.379,\n",
       " 0.379,\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.381,\n",
       " 0.381,\n",
       " 0.381,\n",
       " 0.382,\n",
       " 0.382,\n",
       " 0.382,\n",
       " 0.382,\n",
       " 0.382,\n",
       " 0.383,\n",
       " 0.383,\n",
       " 0.383,\n",
       " 0.384,\n",
       " 0.384,\n",
       " 0.384,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.385,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.386,\n",
       " 0.387,\n",
       " 0.387,\n",
       " 0.387,\n",
       " 0.388,\n",
       " 0.388,\n",
       " 0.388,\n",
       " 0.388,\n",
       " 0.389,\n",
       " 0.389,\n",
       " 0.389,\n",
       " 0.39,\n",
       " 0.39,\n",
       " 0.39,\n",
       " 0.39,\n",
       " 0.39,\n",
       " 0.39,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.391,\n",
       " 0.392,\n",
       " 0.392,\n",
       " 0.392,\n",
       " 0.392,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.393,\n",
       " 0.394,\n",
       " 0.394,\n",
       " 0.394,\n",
       " 0.394,\n",
       " 0.394,\n",
       " 0.395,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.396,\n",
       " 0.397,\n",
       " 0.397,\n",
       " 0.397,\n",
       " 0.397,\n",
       " 0.397,\n",
       " 0.397,\n",
       " 0.398,\n",
       " 0.398,\n",
       " 0.398,\n",
       " 0.398,\n",
       " 0.398,\n",
       " 0.398,\n",
       " 0.399,\n",
       " 0.399,\n",
       " 0.399,\n",
       " 0.399,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.401,\n",
       " 0.402,\n",
       " 0.402,\n",
       " 0.402,\n",
       " 0.402,\n",
       " 0.403,\n",
       " 0.403,\n",
       " 0.403,\n",
       " 0.404,\n",
       " 0.404,\n",
       " 0.404,\n",
       " 0.404,\n",
       " 0.404,\n",
       " 0.404,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.405,\n",
       " 0.406,\n",
       " 0.406,\n",
       " 0.406,\n",
       " 0.406,\n",
       " 0.406,\n",
       " 0.406,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.407,\n",
       " 0.408,\n",
       " 0.408,\n",
       " 0.408,\n",
       " 0.409,\n",
       " 0.409,\n",
       " 0.409,\n",
       " 0.409,\n",
       " 0.409,\n",
       " 0.409,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.41,\n",
       " 0.411,\n",
       " 0.411,\n",
       " 0.411,\n",
       " 0.411,\n",
       " 0.411,\n",
       " 0.411,\n",
       " 0.412,\n",
       " 0.412,\n",
       " 0.412,\n",
       " 0.412,\n",
       " 0.412,\n",
       " 0.413,\n",
       " 0.413,\n",
       " 0.413,\n",
       " 0.413,\n",
       " 0.413,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.414,\n",
       " 0.415,\n",
       " 0.415,\n",
       " 0.415,\n",
       " 0.416,\n",
       " 0.416,\n",
       " 0.416,\n",
       " 0.416,\n",
       " 0.417,\n",
       " 0.417,\n",
       " 0.417,\n",
       " 0.417,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.418,\n",
       " 0.419,\n",
       " 0.419,\n",
       " 0.419,\n",
       " 0.419,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.42,\n",
       " 0.421,\n",
       " 0.421,\n",
       " 0.421,\n",
       " 0.421,\n",
       " 0.421,\n",
       " 0.421,\n",
       " 0.422,\n",
       " 0.422,\n",
       " 0.422,\n",
       " 0.422,\n",
       " 0.423,\n",
       " 0.423,\n",
       " 0.423,\n",
       " 0.423,\n",
       " 0.423,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.424,\n",
       " 0.425,\n",
       " 0.425,\n",
       " 0.425,\n",
       " 0.425,\n",
       " 0.425,\n",
       " 0.425,\n",
       " 0.426,\n",
       " 0.426,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.427,\n",
       " 0.428,\n",
       " 0.428,\n",
       " 0.428,\n",
       " 0.428,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.429,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.43,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.431,\n",
       " 0.432,\n",
       " 0.432,\n",
       " 0.432,\n",
       " 0.432,\n",
       " 0.433,\n",
       " 0.433,\n",
       " 0.433,\n",
       " 0.433,\n",
       " 0.433,\n",
       " 0.433,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.434,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.435,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.436,\n",
       " 0.437,\n",
       " 0.437,\n",
       " 0.437,\n",
       " 0.437,\n",
       " 0.437,\n",
       " 0.438,\n",
       " 0.438,\n",
       " 0.438,\n",
       " 0.438,\n",
       " 0.438,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.439,\n",
       " 0.44,\n",
       " 0.44,\n",
       " 0.44,\n",
       " 0.44,\n",
       " 0.44,\n",
       " 0.441,\n",
       " 0.441,\n",
       " 0.441,\n",
       " 0.442,\n",
       " 0.442,\n",
       " 0.443,\n",
       " 0.443,\n",
       " 0.443,\n",
       " 0.443,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.444,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.445,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.446,\n",
       " 0.447,\n",
       " 0.447,\n",
       " 0.447,\n",
       " 0.447,\n",
       " 0.447,\n",
       " 0.447,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.448,\n",
       " 0.449,\n",
       " 0.449,\n",
       " 0.449,\n",
       " 0.449,\n",
       " 0.449,\n",
       " 0.45,\n",
       " 0.45,\n",
       " 0.45,\n",
       " 0.45,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.451,\n",
       " 0.452,\n",
       " 0.452,\n",
       " 0.452,\n",
       " 0.452,\n",
       " 0.452,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.453,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.454,\n",
       " 0.455,\n",
       " 0.455,\n",
       " 0.455,\n",
       " 0.455,\n",
       " 0.455,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.456,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.457,\n",
       " 0.458,\n",
       " 0.458,\n",
       " 0.458,\n",
       " 0.458,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.459,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.461,\n",
       " 0.461,\n",
       " 0.461,\n",
       " 0.461,\n",
       " 0.462,\n",
       " 0.462,\n",
       " 0.462,\n",
       " 0.462,\n",
       " 0.462,\n",
       " 0.462,\n",
       " 0.463,\n",
       " 0.463,\n",
       " 0.463,\n",
       " 0.463,\n",
       " 0.463,\n",
       " 0.463,\n",
       " ...]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(simi_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "efcae35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Harish'\n",
    "text2 = 'Gola'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00e6ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "19286ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text1'] = [text1]\n",
    "data['text2'] = [text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d34097e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harish</td>\n",
       "      <td>Gola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text1 text2\n",
       "0  Harish  Gola"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c28c2269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set values for various parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m feature_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m    \u001b[38;5;66;03m# Word vector dimensionality\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ft_model1 \u001b[38;5;241m=\u001b[39m FastText(data\u001b[38;5;241m.\u001b[39mtext1\u001b[38;5;241m.\u001b[39mto_list(), vector_size\u001b[38;5;241m=\u001b[39mfeature_size, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m ft_model2 \u001b[38;5;241m=\u001b[39m FastText(data\u001b[38;5;241m.\u001b[39mtext2\u001b[38;5;241m.\u001b[39mto_list(), vector_size\u001b[38;5;241m=\u001b[39mfeature_size, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\fasttext.py:435\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors_vocab_lockf \u001b[38;5;241m=\u001b[39m ones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors_ngrams_lockf \u001b[38;5;241m=\u001b[39m ones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28msuper\u001b[39m(FastText, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    436\u001b[0m     sentences\u001b[38;5;241m=\u001b[39msentences, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, workers\u001b[38;5;241m=\u001b[39mworkers, vector_size\u001b[38;5;241m=\u001b[39mvector_size, epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    437\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks, batch_words\u001b[38;5;241m=\u001b[39mbatch_words, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule, sg\u001b[38;5;241m=\u001b[39msg, alpha\u001b[38;5;241m=\u001b[39malpha, window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[0;32m    438\u001b[0m     max_vocab_size\u001b[38;5;241m=\u001b[39mmax_vocab_size, max_final_vocab\u001b[38;5;241m=\u001b[39mmax_final_vocab,\n\u001b[0;32m    439\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count, sample\u001b[38;5;241m=\u001b[39msample, sorted_vocab\u001b[38;5;241m=\u001b[39msorted_vocab,\n\u001b[0;32m    440\u001b[0m     null_word\u001b[38;5;241m=\u001b[39mnull_word, ns_exponent\u001b[38;5;241m=\u001b[39mns_exponent, hashfxn\u001b[38;5;241m=\u001b[39mhashfxn,\n\u001b[0;32m    441\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed, hs\u001b[38;5;241m=\u001b[39mhs, negative\u001b[38;5;241m=\u001b[39mnegative, cbow_mean\u001b[38;5;241m=\u001b[39mcbow_mean,\n\u001b[0;32m    442\u001b[0m     min_alpha\u001b[38;5;241m=\u001b[39mmin_alpha, shrink_windows\u001b[38;5;241m=\u001b[39mshrink_windows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha \u001b[38;5;241m=\u001b[39m end_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m-> 1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_training_sanity(epochs\u001b[38;5;241m=\u001b[39mepochs, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words)\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1050\u001b[0m     msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     ),\n\u001b[0;32m   1055\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\word2vec.py:1543\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1540\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffective \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m higher than previous training cycles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index:  \u001b[38;5;66;03m# should be set by `build_vocab`\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must first build vocabulary before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors):\n\u001b[0;32m   1545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must initialize vectors before training the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality\n",
    "ft_model1 = FastText(data.text1.to_list(), vector_size=feature_size, sg=1)\n",
    "ft_model2 = FastText(data.text2.to_list(), vector_size=feature_size, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "df40c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 'broadband challenges tv viewing the number of europeans with broadband has exploded over the past 12 months  with the web eating into tv viewing habits  research suggests.  just over 54 million people are hooked up to the net via broadband  up from 34 million a year ago  according to market analysts nielsen/netratings. the total number of people online in europe has broken the 100 million mark. the popularity of the net has meant that many are turning away from tv  say analysts jupiter research. it found that a quarter of web users said they spent less time watching tv in favour of the net  the report by nielsen/netratings found that the number of people with fast internet access had risen by 60% over the past year.  the biggest jump was in italy  where it rose by 120%. britain was close behind  with broadband users almost doubling in a year. the growth has been fuelled by lower prices and a wider choice of always-on  fast-net subscription plans.  twelve months ago high speed internet users made up just over one third of the audience in europe; now they are more than 50% and we expect this number to keep growing   said gabrielle prior  nielsen/netratings analyst.  as the number of high-speed surfers grows  websites will need to adapt  update and enhance their content to retain their visitors and encourage new ones.  the total number of europeans online rose by 12% to 100 million over the past year  the report showed  with the biggest rise in france  italy  britain and germany.  the ability to browse web pages at high speed  download files such as music or films and play online games is changing what people do in their spare time.  a study by analysts jupiter research suggested that broadband was challenging television viewing habits. in homes with broadband  40% said they were spending less time watching tv. the threat to tv was greatest in countries where broadband was on the up  in particular the uk  france and spain  said the report. it said tv companies faced a major long-term threat over the next five years  with broadband predicted to grow from 19% to 37% of households by 2009.  year-on-year we are continuing to see a seismic shift in where  when and how europe s population consume media for information and entertainment and this has big implications for tv  newspaper and radio   said jupiter research analyst olivier beauvillian.'\n",
    "t2 = 'gardener wins double in glasgow britain s jason gardener enjoyed a double 60m success in glasgow in his first competitive outing since he won 100m relay gold at the athens olympics.  gardener cruised home ahead of scot nick smith to win the invitational race at the norwich union international. he then recovered from a poor start in the second race to beat swede daniel persson and italy s luca verdecchia. his times of 6.61 and 6.62 seconds were well short of american maurice greene s 60m world record of 6.39secs from 1998.  it s a very hard record to break  but i believe i ve trained very well   said the world indoor champion  who hopes to get closer to the mark this season.  it was important to come out and make sure i got maximum points. my last race was the olympic final and there was a lot of expectation.  this was just what i needed to sharpen up and get some race fitness. i m very excited about the next couple of months.   double olympic champion  marked her first appearance on home soil since winning 1500m and 800m gold in athens with a victory. there was a third success for britain when  edged out russia s olga fedorova and sweden s jenny kallur to win the women s 60m race in 7.23secs. maduaka was unable to repeat the feat in the 200m  finishing down in fourth as  took the win for russia. and the 31-year-old also missed out on a podium place in the 4x200m relay as the british quartet came in fourth  with russia setting a new world indoor record. there was a setback for jade johnson as she suffered a recurrence of her back injury in the long jump. russia won the meeting with a final total of 63 points  with britain second on 48 and france one point behind in third.  led the way for russia by producing a major shock in the high jump as he beat olympic champion stefan holm into second place to end the swede s 22-event unbeaten record.  won the triple jump with a leap of 16.87m  with britain s tosin oke fourth in 15.80m.  won the men s pole vault competition with a clearance of 5.65m  with britain s nick buckfield 51cm adrift of his personal best in third. and  won the women s 800m  with britain s jenny meadows third. there was yet another russian victory in the women s 400m as  finished well clear of britain s catherine murphy. chris lambert had to settle for fourth after fading in the closing stages of the men s 200m race as sweden s  held off leslie djhone of france. france s  won the men s 400m  with brett rund fourth for britain.  took victory for sweden in the women s 60m hurdles ahead of russia s irina shevchenko and britain s sarah claxton  who set a new personal best. italy grabbed their first victory in the men s 1500m as  kicked over the last 200 metres to hold off britain s james thie and france s alexis abraham. a botched changeover in the 4x200m relay cost britain s men the chance to add further points as france claimed victory.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "49d5be4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rap boss arrested over drug find rap mogul marion  suge  knight has been arrested for violating his parole after he was allegedly found with marijuana.  he was arrested in barstow  california  on saturday following an alleged traffic offence. he is expected to be transferred to a state prison while a decision is made on whether he should be released. mr knight  founder of death row records  served a 10-month jail term in 2004 for punching a man while on parole for an assault conviction. police said mr knight was stopped on saturday after performing an illegal u-turn and a search of his car allegedly found marijuana.  he is also accused of not having insurance. a 18-year-old woman in the car was arrested for providing false information and having a fake id card. she was later released. it was his second alleged violation  having previously served half of a nine-year sentence for breaking the terms of his parole. mr knight  39  was jailed in october 1996 following his involvement in a fight with a rival gang just hours before rapper tupac shakur was killed in a las vegas drive-by shooting. he was driving shakur s car at the time and was shot in the head. at the time he was on probation for assaulting two musicians. mr knight  a former bodyguard  set up death row records in the early 1990s with shakur and dr dre among his protegees. but the label has always been dogged by allegations it supports gang culture and fuels the east and west coast rap rivalry.',\n",
       " 'amnesty chief laments war failure the lack of public outrage about the war on terror is a powerful indictment of the failure of human rights groups  amnesty international s chief has said.  in a lecture at the london school of economics  irene khan said human rights had been flouted in the name of security since 11 september  2001. she said the human rights movement had to use simpler language both to prevent scepticism and spread a moral message. and it had to fight poverty  not just focus on political rights for elites.  ms khan highlighted detentions without trial  including those at the us camp at guantanamo bay in cuba  and the abuse of prisoners as evidence of increasing human rights problems.  what s a new challenge is the way in which this age-old debate on security and human rights has been translated into the language of war   she said.  by using the language of war  human rights are being sidelined because we know human rights do not apply in times of war.  ms khan said such breaches were infectious and were now seen in almost very major country in the world.  the human rights movement faces a crisis of faith in the value of human rights   she said. that was accompanied by a crisis of governance  where the united nations system did not seem able to hold countries to account.  the amnesty secretary-general said a growing gap between the perceived influence of human rights group and what they could actually achieve was fuelling scepticism.  public passivity on the war against terror is the single most powerful indictment on the failures of human rights groups   she said. ms khan said the movement had failed to mobilise public outrage about what was happening to the human rights system. there needed to be a drive to use simpler language  talking about the basic morality of the issues rather than the complexity of legal processes. such efforts could make the issues more relevant to people across the world  she said.  the human rights groups also had to recognise there were new groups which had to be tackled in new ways as power dripped away from state governments. al-qaeda  for example  was not going to be impressed by a traditional amnesty letter writing campaign. more also needed to be done to develop a human rights framework for international business corporations. amnesty international members voted in 2001 to extend the organisation s work from political and civil rights to cover social and economic rights too. ms khan said the human rights movement would make itself irrelevant if it turned away from the suffering caused by economic strife.  we would be an elitist bunch working for the elites  for those who cannot read the newspaper of their choice rather than those who cannot read   she said. despite her concerns  ms khan dubbed herself a  hope-monger   saying she was confident the passions of the human rights movement could overcome the new challenges.']"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'rap boss arrested over drug find rap mogul marion  suge  knight has been arrested for violating his parole after he was allegedly found with marijuana.  he was arrested in barstow  california  on saturday following an alleged traffic offence. he is expected to be transferred to a state prison while a decision is made on whether he should be released. mr knight  founder of death row records  served a 10-month jail term in 2004 for punching a man while on parole for an assault conviction. police said mr knight was stopped on saturday after performing an illegal u-turn and a search of his car allegedly found marijuana.  he is also accused of not having insurance. a 18-year-old woman in the car was arrested for providing false information and having a fake id card. she was later released. it was his second alleged violation  having previously served half of a nine-year sentence for breaking the terms of his parole. mr knight  39  was jailed in october 1996 following his involvement in a fight with a rival gang just hours before rapper tupac shakur was killed in a las vegas drive-by shooting. he was driving shakur s car at the time and was shot in the head. at the time he was on probation for assaulting two musicians. mr knight  a former bodyguard  set up death row records in the early 1990s with shakur and dr dre among his protegees. but the label has always been dogged by allegations it supports gang culture and fuels the east and west coast rap rivalry.,amnesty chief laments war failure the lack of public outrage about the war on terror is a powerful indictment of the failure of human rights groups  amnesty international s chief has said.  in a lecture at the london school of economics  irene khan said human rights had been flouted in the name of security since 11 september  2001. she said the human rights movement had to use simpler language both to prevent scepticism and spread a moral message. and it had to fight poverty  not just focus on political rights for elites.  ms khan highlighted detentions without trial  including those at the us camp at guantanamo bay in cuba  and the abuse of prisoners as evidence of increasing human rights problems.  what s a new challenge is the way in which this age-old debate on security and human rights has been translated into the language of war   she said.  by using the language of war  human rights are being sidelined because we know human rights do not apply in times of war.  ms khan said such breaches were infectious and were now seen in almost very major country in the world.  the human rights movement faces a crisis of faith in the value of human rights   she said. that was accompanied by a crisis of governance  where the united nations system did not seem able to hold countries to account.  the amnesty secretary-general said a growing gap between the perceived influence of human rights group and what they could actually achieve was fuelling scepticism.  public passivity on the war against terror is the single most powerful indictment on the failures of human rights groups   she said. ms khan said the movement had failed to mobilise public outrage about what was happening to the human rights system. there needed to be a drive to use simpler language  talking about the basic morality of the issues rather than the complexity of legal processes. such efforts could make the issues more relevant to people across the world  she said.  the human rights groups also had to recognise there were new groups which had to be tackled in new ways as power dripped away from state governments. al-qaeda  for example  was not going to be impressed by a traditional amnesty letter writing campaign. more also needed to be done to develop a human rights framework for international business corporations. amnesty international members voted in 2001 to extend the organisation s work from political and civil rights to cover social and economic rights too. ms khan said the human rights movement would make itself irrelevant if it turned away from the suffering caused by economic strife.  we would be an elitist bunch working for the elites  for those who cannot read the newspaper of their choice rather than those who cannot read   she said. despite her concerns  ms khan dubbed herself a  hope-monger   saying she was confident the passions of the human rights movement could overcome the new challenges.'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "b9e437fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>stemmed_text1</th>\n",
       "      <th>stemmed_text2</th>\n",
       "      <th>text1_vec</th>\n",
       "      <th>text2_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "      <td>[broadband, challenge, tv, viewing, number, eu...</td>\n",
       "      <td>[gardener, win, double, glasgow, britain, jaso...</td>\n",
       "      <td>(0, 15)\\t1.0\\n  (1, 18)\\t1.0\\n  (2, 133)\\t1....</td>\n",
       "      <td>(0, 59)\\t1.0\\n  (1, 182)\\t1.0\\n  (2, 42)\\t1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenges tv viewing the number of ...   \n",
       "\n",
       "                                               text2  \\\n",
       "0  gardener wins double in glasgow britain s jaso...   \n",
       "\n",
       "                                       stemmed_text1  \\\n",
       "0  [broadband, challenge, tv, viewing, number, eu...   \n",
       "\n",
       "                                       stemmed_text2  \\\n",
       "0  [gardener, win, double, glasgow, britain, jaso...   \n",
       "\n",
       "                                           text1_vec  \\\n",
       "0    (0, 15)\\t1.0\\n  (1, 18)\\t1.0\\n  (2, 133)\\t1....   \n",
       "\n",
       "                                           text2_vec  \n",
       "0    (0, 59)\\t1.0\\n  (1, 182)\\t1.0\\n  (2, 42)\\t1....  "
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://www.google.com/url?sa=i&url=https%3A%2F%2Fpngtree.com%2Ffree-backgrounds&psig=AOvVaw0-UVnESOpyx2SEU8jS8OW5&ust=1705663136874000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCLj5nsDo5oMDFQAAAAAdAAAAABAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76679304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
